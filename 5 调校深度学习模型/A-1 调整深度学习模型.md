#### 产生过度拟合的原因

​	训练得到的模型太复杂，记住太多训练资料集的杂讯

#### 避免产生过度拟合

* 增加训练资料集的资料量

  * 目的是增加训练资料多样性

* 使用资料增强技术 Data Augmentation

* 减少模型复杂度

  * 删除一些隐藏层的神经层
  * 在神经层减少神经元数

* 使用Dropout层

  * 在模型中增加更多的Dropout层
  * 增加Dropout层权重归零的比例：从0.5→0.75

* 提早停止训练周期 Early Stopping Epoch

  * 除手动减少训练周期外，当准确度不再提高时，使用Keras的EarlyStopping

* L1 和 L2 常规化  L1 and L2 Regularization

  * L1 和 L2 常规化是一种权重衰减（Weight Decay）观念，在损失函数中增加 惩罚权重项，从而增加损失，使反向传播的权重更新调整更多

  * Y：真实目标值，  Y'：预测值，  Y' = W*X，

    W：权重，  X：训练资料，  wd：权重衰减率

    * L1 常规化：Loss = （Y' - Y）^2 + wd*Abs（W）
    * L2 常规化：Loss = （Y' - Y）^2 + wd*（W）^2

#### L1，L2常规化实作

```python
from tensorflow.keras import regularizers

# 建立常规化物件

regularizers.l1(0.01) # 或
regularizers.l2(0.02) # 或
regularizers.l1_l2(l1=0.01,l2=0.01) # 或

# 在Dense、Conv2D、LSTM等神经层使用L1和L2常规化

model.add(Conv2D(64,kernel_size=(3,3),padding="same",
                 activation="relu",
                 kernel_regularizer=regularizers.l2(0.02), # 指定损失函数的L1/L2常规化
                 bias_regularizer=regularizers.l2(0.02))   # 指定偏向量的L1/L2常规化
          
```

#### 避免低度拟合Undefitting

* 增加模型复杂度
  * 增加神经层数
  * 每一层增加神经元数
  * 使用不同的神经层种类
* 减少Dropout层
  * 调低随即归零比例或者删除Dropout层
* 增加样本资料的特征数
  * 如使用高度、宽度来分类，再加上额外的色彩特征
  * 如股票预测原来只有收盘价，现在增加开盘价、最高价、最低价、成交



#### 如何加速神经网络的训练

* 选择优化器
* 批次正规化

#### 优化器（Optimizer）

* 功能：更新神经网络的权重来让损失函数的误差值最小化，找出神经网络的最佳权重

* 做法：使用反向传播计算出每一层权重要分担损失的梯度后，使用梯度下降法来更新每一层权重

  ​				公式：W1 = W0 - 学习率 * 梯度

* 加速训练：

  * 调整学习率步伐大小

  * 增加一些参数，如动量（Momentum）：

    ​		 优化后的公式： W1 = W0 - （学习率 * 梯度 + 动量）

    通过调整学习率和动量，加速神经网络训练，使其更快收敛（Converge)

#### 动量

​	动量来自物理学惯性，同一方向加速，更改方向减速

* 当梯度方向和上一次更新量方向相同，可以从上一次更新量得到加速作用
* 当梯度方向和上一次更新量方向相反，可以从上一次更新量得到减速作用

公式：

​	V1 = - lr * 梯度 + V0 * momentum

​	W1 = W0 + V1

​	其中：V1是这一次的更新量，V0是上一次的更新量，lr是学习率，momentum是动量

综上：动量的使用不只可以加快收敛，还可以减少收敛时发生的震荡

#### 学习率衰减系数 Learning Rate Decay

​	刚开始训练使用大步伐，随着训练增加，越来越接近最小值时，步伐变小，从而减少收敛时的震荡

​	公式：

​			lr1 = lr0 * 1.0 /（1.0 + decay * 更新次数）

#### 自适应性学习率 Adaptive Learning Rates

* Adagrad：初期梯度较小时，能做较大更新，后期训练梯度较大时，做较小更新。问题是分母会不断积累，最后导致学习率极速下降，最终变得非常小
* Adadelta：修改了Adagrad分式的分母问题，换成了过去梯度平方的衰减平均值，可以改进Adagrad学习率急剧下降问题
* RMSprop：增加了一个衰减系统，能够根据之前每一次梯度变化情况来自动更新学习率，缓解Adagrad学习率急剧下降问题
* Adam：Adam是Adagrad和momentum的结合体，保留Adagrad依据梯度自动调整学习率，和momentum对梯度方向的惯性调整，而且会执行偏差校正，让学习率有一个确定范围，更平稳的进行参数更新。Adam和RMSprop差不多，整体来说Adam更好

#### 使用自定义Keras优化器

* SGD优化器：（最基本的优化器，MBGD，Mini-Batch Gradient Descent，最小批次梯度下降）

  ```python
  from tensorflow.keras import optimizers
  
  opt_sgd = optimizers.SGD(lr=0.05, momentum=.09, decay=1e-6) # 分别是学习率、动量、衰减比率
  
  model.compile(
  	loss = "categorical_crossentropy",
      optimizer = opt_sgd, #这里不再使用原来的已定义字符串
      metrics = ["accuracy"]
  )
  ```

* Adam优化器：

  ```python
  from tensorflow.keras import optimizers
  
  opt_adam = optimizers.Adam(lr=0.001, decay=1e-6)
  
  model.compile(
  	loss = "categorical_crossentropy",
      optimizer = opt_adam,
      metrics = ["accuracy"]
  )
  ```

* RMSprop优化器：

  ```python
  from tensorflow.keras import optimizers
  
  opt_rms = optimizers.Adam(lr=0.001, decay=1e-6)
  
  model.compile(
  	loss = "categorical_crossentropy",
      optimizer = opt_rms,
      metrics = ["accuracy"]
  )
  ```

#### 批次正规化

​	批次正规化（Batch Normalization，简称BN）和 特征标准化 相似，差别是输入资料标准化后在后续调整，再次变地太大或者太小，原始论文中称为：内部共变量位移（Internal Covariate Shift）

​	这样做是为了加大模型对数据的敏感度，将资料分布在启动函数的敏感区间

优点：

* 加速训练，更快收敛
* 可以在优化器使用更大学习率，并且让初始权重更加简单
* 缓解梯度消失，在神经层也可使用更多种类的启动函数


