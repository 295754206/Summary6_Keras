#### 资料向量化

资料向量化（Data Vectorization）是把声音、图片、文字资料转换成数值资料的张量，对于自然语言处理而言，就是文字资料向量化

#### 文字资料的One-hot编码

* 英文单词的 One-hot 编码

  ```python
  import numpy as np
  
  samples = ["I hate this movie",
             "This movie is not good"]
  
  # 设置空字典
  
  token_index = {}  
  
  # 分割文本中单词成为列表的函数
  
  def word_tokenize(tx):
      tx = tx.lower()
      return tx.split()
  
  # 填写字典
  
  for text in samples:
      for word in word_tokenize(text):
          if word not in token_index:
              token_index[word] = len(token_index) + 1
  
  print(token_index)
  
  # 设置单句最大单字长度：往往是最长句子长度+1，因为0号位常空置不用
  
  max_length = 6
  
  # 制造一个(2,6,7)的三维0数组，一个二维数组就是一句话，第三维的每一个向量就是每一个单字
  
  results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))
  
  for i, text in enumerate(samples):
      words = list(enumerate(word_tokenize(text)))[:max_length]  # 只截取最大单句长度内的字
      for j, word in words:
          index = token_index.get(word)
          results[i, j, index] = 1.0  # 原本位置的标记位置设置为1
  
  print(results[0])
  print(results[1])
  ```
  
  ```python
  {'i': 1, 'hate': 2, 'this': 3, 'movie': 4, 'is': 5, 'not': 6, 'good': 7}
  [[0. 1. 0. 0. 0. 0. 0. 0.]
   [0. 0. 1. 0. 0. 0. 0. 0.]
   [0. 0. 0. 1. 0. 0. 0. 0.]
   [0. 0. 0. 0. 1. 0. 0. 0.]
   [0. 0. 0. 0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0. 0. 0. 0.]]
  [[0. 0. 0. 1. 0. 0. 0. 0.]
   [0. 0. 0. 0. 1. 0. 0. 0.]
   [0. 0. 0. 0. 0. 1. 0. 0.]
   [0. 0. 0. 0. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0. 0. 0. 1.]
   [0. 0. 0. 0. 0. 0. 0. 0.]]
  ```

#### 词向量与词嵌入

* 词向量（Word Vector）或称 词嵌入（ Word Embedding），也是一种文字资料向量化的方法，可以将单字嵌入一个浮点数的数学空间里

* 区别：
  * One-hot 是使用代码转换成单字来成为向量
  * 词向量是建立神经网络来自行学习单字的词向量
* 以10000个word为例，分别使用 One-hot 和 词向量（使用200个神经元的隐藏层）执行文字资料向量化的差异：
  *  one-hot 需要 10000 x 10000 的 稀疏矩阵
  * 词向量是低维度浮点数的紧密矩阵，第一层输入是10000个输入点（输入不属于该神经网络），第二层是200个神经元（200个维度：例如 狗、狼都属于犬类，猫、虎都属于猫科，狗→狼 和 猫→虎 表示从 家养→野生，狗→猫 和 狼→虎 表示从 犬科→猫科），第三层还是10000个输出点（表示10000个字的输出概率）。上述神经网络隐藏层（第二层）没有启动函数，输出层使用Softmax函数输出各预测单字的几率。
  * 以上面为例，词向量隐藏层权重需要 10000 x 200 的矩阵

#### CBOW 和 Skip-gram 模型

* CBOW  使用周围字来预测 中间字
* Skip-gram 源于 N-gram 模型，使用一个单字来预测周边字
