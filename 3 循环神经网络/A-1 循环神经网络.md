#### 循环神经网络（RNN）

* 图示：

  ​               输入向量 **X t**                                                                      输出向量**Y t**

  ​			            **+**                                            **→**tanh**→**                             **+** 

  ​       上一时步 t-1 的隐藏层输出 **h t-1**                               下一时步 t+1 的隐藏层输出 **h t+1**





​				输出层：                   **Y  t-1**                              **Y  t**                            **Y  t+1**

​										        	 ↑ V                                ↑ V                               ↑ V

​				隐藏层： --W-->      **h  t-1**          --W-->         **h  t+1**       --W-->        **h  t+1 **      --W--> 

​    											     ↑ U                                ↑ U                              ↑ U

​				输入层：                   **X  t-1**                               **X t**                            **X  t+1**



* 注意：循环神经网络每一时步的神经网络权重 U、V 和 W 都是权重共享

* 权重的公式计算：

  ​	h1 = f（U x X1 + W x h0）                  f（）是隐藏层的启动函数，可以使用 Tanh、ReLU、Sigmoid函数

  ​	y1 = g（V x h1）								  g（）是输出层的启动函数，如果是分类问题一般使用Softmax函数

* Y i 是预测值，T i是标签值，RNN的全部损失E:

  ​    E = Σ (i=1→t) f（Y i - T i）

* 反向传播：透过时间的反向传播算法（Backpropagation Through Time，BPTT）

#### 循环神经网络的种类

* 一对多
  * 一对多只有一个输入 X ，然后每一时步产生的 Y t 作为下一时步的输入（X t），每一时步都会有输出，最终产生完整连续的输出
  * 例如：输入一张图片，输出一段文字或者音乐
* 多对一
  * 每一个时步都有X t ，但是我们只需要最后一步的最终输出 Y 
  * 例如：输入影评，输出正面或负面评价
* 多对多
  * 输入和输出等长：
    * 这种类型即上面图例所示的完全神经网络，每一个输入都有一个输出
    * 例如：判断每一个单字是否是一个人名
  * 输入和输出不等长：
    * 输入和输出均不对等，常用于机器翻译
    * 例如：将中文翻译成英文，句子往往不等长

#### 梯度消失和梯度爆炸

* RNN只有短期记忆，步数较长会发生梯度消失问题
* 如果反向传播计算出的梯度小于1，产生梯度消失问题，只能用LSTM和GRU来解决
* 如果反向传播计算出的梯度大于1，产生梯度爆炸问题，可以使用神经网络最佳化来解决这个问题

#### 长短期记忆神经网络（LSTM）

* Long Short-term Memory 改良自 RNN，是一种拥有长期记忆能力的神经网络
* 遗忘闸、输入闸、输出闸

#### 闸门循环单元神经网络（GRU）

* Gate Recurrent Unit 是 LSTM 的更新版，结构更简单、速度更快、减少记忆体的使用
* 重设闸、更新闸、Tanh神经层

#### RNN、LSTM、GRU层的参数计算

* RNN层 多对一

  ```python
  from tensorflow.keras.layers import Embedding
  from tensorflow.keras.layers import SimpleRNN  
  from tensorflow.keras.models import Sequential
  
  model = Sequential()
  model.add(Embedding(10000, 32, input_length=100))
  model.add(SimpleRNN(32)) 
  model.summary()
  ```

  SimpleRNN层：

  ​	上一层输出通道  x  核心数  x  2（RNN有U和W两种权重）

  ​	即：32x32x2+32 = 2080

* LSTM层  多对多

  ```python
  from tensorflow.keras.layers import Embedding
  from tensorflow.keras.layers import LSTM  
  from tensorflow.keras.models import Sequential
  
  model = Sequential()
  model.add(Embedding(10000, 32, input_length=100))
  model.add(LSTM(32, return_sequences=True)) 
  model.summary()
  ```

  ​	第二个参数 return_sequences 一般默认是flase，即只出最后一个结果而不是整个所有输出

  LSTM层 ：

  ​	LSTM 参数量  = （特征数 + 单元数）x（单元数 x 4）+（单元数 x 4）

  ​	即：(32 + 32) x (32 x 4) + (32 x 4) = 8320

* GRU层 多个堆叠的循环神经网络

  ```python
  from tensorflow.keras.layers import Embedding
  from tensorflow.keras.layers import GRU  
  from tensorflow.keras.models import Sequential
  
  model = Sequential()
  model.add(Embedding(10000, 32, input_length=100))
  model.add(GRU(32, return_sequences=True))
  model.add(GRU(32, return_sequences=True))
  model.add(GRU(32))
  model.summary()
  ```

  GRU层：

  ​	GRU 参数量  = （特征数 + 单元数）x（单元数 x 3）+（单元数 x 3）

  ​	即：（32 x 32）x（32 x 3）+（32 x 3）= 6240


